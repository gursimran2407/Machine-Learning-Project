{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "\n",
    "class entryPoint():\n",
    "\n",
    "    def printaccuracy(self,y_test,predict,model):\n",
    "        print(model,\" report\")\n",
    "        print(\"-------------------------------------\")\n",
    "        print(\" \")\n",
    "        print(\" Confusion Matrix \" ,confusion_matrix(y_test,predict))\n",
    "        print(classification_report(y_test,predict))\n",
    "        print(\" \")\n",
    "        print(\"-------------------------------------\")\n",
    "        print(\" \")\n",
    "    \n",
    "    def normalizedata(self,X):\n",
    "        SS = StandardScaler()\n",
    "        X = SS.fit_transform(X)\n",
    "        print(\"Normalization done\")\n",
    "        return X\n",
    "\n",
    "    def removeoutliers(self,data,inplace=False):\n",
    "        prev_rows = len(data)\n",
    "        data_copy = data.copy()\n",
    "        z_score = np.abs(stats.zscore(data_copy))\n",
    "        data_copy = data_copy[(z_score < 3).all(axis=1)]\n",
    "        if inplace:\n",
    "            data=data_copy\n",
    "        print(\"Before removing outliers , rows - \", prev_rows)\n",
    "        print(\"After removing outliers , rows -\", len(data_copy))\n",
    "        print(\"Number of records deleted - \", (prev_rows - len(data_copy)))\n",
    "        return data_copy\n",
    "    \n",
    "    def PC(self,components,x):\n",
    "        cols = []\n",
    "        pca = PCA(n_components=components)\n",
    "        pc = pca.fit_transform(x)\n",
    "        for i in range(components):\n",
    "            cols.append('pc'+str(i))\n",
    "        pc_data = pd.DataFrame(data = pc, columns = cols)\n",
    "        return pc_data\n",
    "\n",
    "\n",
    "    def train_split(self,X,y,test_size=0.2,random_state=0):\n",
    "        X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)\n",
    "        return X_train,X_test,y_train,y_test\n",
    "\n",
    "    def knn(self,X_train,y_train,X_test,y_test):\n",
    "        print(\"Knn\")\n",
    "        knn_error = []\n",
    "        for i in range(2,10):\n",
    "            knn = KNeighborsClassifier(n_neighbors=i)\n",
    "            knn.fit(X_train,y_train)\n",
    "            knn_predict= knn.predict(X_test)\n",
    "        print(type(knn_predict))\n",
    "        print(type(y_test))\n",
    "        knn_error.append(np.mean(y_test!=knn_predict))\n",
    "        plt.plot(range(2,50),knn_error)\n",
    "        plt.xlabel(\"K value\")\n",
    "        plt.ylabel(\"Error\")\n",
    "    \n",
    "    def knn_grid_search(self,X_train,y_train,X_test,y_test,inp_params):\n",
    "        print(\"Knn Grid Search Starting...\")\n",
    "        knn_grid=GridSearchCV(KNeighborsClassifier(),inp_params,verbose=False,refit=True,cv=3)\n",
    "        knn_grid.fit(X_train,y_train)\n",
    "        knn_predict = knn_grid.predict(X_test)\n",
    "        #self.printaccuracy(y_test,knn_predict,\"KNN\")\n",
    "        #print(\"Best Hyperparameters \" + str(knn_grid.best_params_) + \" Best Score: \" + str(knn_grid.best_score_))\n",
    "        res= [knn_grid.score(X_train,y_train),\n",
    "              knn_grid.score(X_test,y_test),\n",
    "              precision_score(y_test,knn_predict,average ='weighted'),\n",
    "              recall_score(y_test,knn_predict,average ='weighted'),f1_score(y_test,knn_predict,average ='weighted')]\n",
    "        result = pd.DataFrame(np.array(res).reshape(-1,5))\n",
    "        return result\n",
    "    \n",
    "    def logisticRegression(self,X_train,y_train,X_test,y_test,inp_params):\n",
    "        print(\"Logistic Regression classification Starting...\")\n",
    "#        score = []\n",
    "#         for pen in penalty_reg:\n",
    "#             for i in Co_reg:\n",
    "#                 for it in max_iteration:\n",
    "#                     clf = LogisticRegression(random_state=0, solver='liblinear', penalty=pen , C=i, max_iter=it).fit(X_train, y_train.values.ravel())\n",
    "#                     score.append(clf.score(X_test, y_test.values.ravel()))\n",
    "        lr = GridSearchCV(LogisticRegression(random_state=0),inp_params,verbose=False,refit=True,cv=3)\n",
    "        lr.fit(X_train,y_train)\n",
    "        lr_predict = lr.predict(X_test)\n",
    "        #self.printaccuracy(y_test,lr_predict,\"Logistic Regression\")\n",
    "        res= [lr.score(X_train,y_train),\n",
    "        lr.score(X_test,y_test),\n",
    "        precision_score(y_test,lr_predict,average ='weighted'),\n",
    "        recall_score(y_test,lr_predict,average ='weighted'),f1_score(y_test,lr_predict,average ='weighted')]\n",
    "        result = pd.DataFrame(np.array(res).reshape(-1,5))\n",
    "        return result\n",
    "        \n",
    "    def svm_model(self,X_train,y_train,X_test,y_test,inp_params):\n",
    "        print(\"SVM Classification Starting...\")\n",
    "        svm = SVC(kernel='rbf',random_state=0)\t\n",
    "        svm_grid = GridSearchCV(svm, inp_params, verbose=False, cv=3,return_train_score=True)\n",
    "        svm_grid.fit(X_train,y_train)\n",
    "        svm_predict = svm_grid.predict(X_test)\n",
    "        #self.printaccuracy(y_test,svm_predict,\"SVM\")\n",
    "        #print(\"Best Hyperparameters \" + str(svm_grid.best_params_) + \" Best Score: \" + str(svm_grid.best_score_))\n",
    "        res= [svm_grid.score(X_train,y_train),\n",
    "           svm_grid.score(X_test,y_test),\n",
    "           precision_score(y_test,svm_predict,average ='weighted'),\n",
    "           recall_score(y_test,svm_predict,average ='weighted'),f1_score(y_test,svm_predict,average ='weighted')]\n",
    "        result = pd.DataFrame(np.array(res).reshape(-1,5))\n",
    "        return result\n",
    "\n",
    "    def decisionTreeClassifier(self,X_train,y_train,X_test,y_test,inp_params):\n",
    "        print(\"Decisiontree Classifier Starting...\")\n",
    "        decisionTree_grid = GridSearchCV(DecisionTreeClassifier(random_state=0), inp_params, verbose=False, cv=3,return_train_score=True)\n",
    "        decisionTree_grid.fit(X_train,y_train)\n",
    "        decisionTree_predict = decisionTree_grid.predict(X_test)\n",
    "        #self.printaccuracy(y_test,decisionTree_predict,\"DecisionTree\")\n",
    "        #print(\"Best Hyperparameters \" + str(decisionTree_predict.best_params_) + \" Best Score: \" + str(decisionTree_predict.best_score_))\n",
    "        res= [decisionTree_grid.score(X_train,y_train),\n",
    "           decisionTree_grid.score(X_test,y_test),\n",
    "           precision_score(y_test,decisionTree_predict,average ='weighted'),\n",
    "           recall_score(y_test,decisionTree_predict,average ='weighted'),f1_score(y_test,decisionTree_predict,average ='weighted')]\n",
    "        result = pd.DataFrame(np.array(res).reshape(-1,5))\n",
    "        return result\n",
    "    \n",
    "    def randomForest(self,X_train,y_train,X_test,y_test,inp_params):\n",
    "        print(\"randomForest Classifier Starting...\")\n",
    "        rf = RandomForestClassifier(random_state=0)\n",
    "        rf_grid = GridSearchCV(rf, inp_params, verbose=False, cv=3)\n",
    "        rf_grid.fit(X_train,y_train)\n",
    "        rf_predict = rf_grid.predict(X_test)\n",
    "        #self.printaccuracy(y_test,rf_predict,\"RandomForest\")\n",
    "        #print(\"Best Hyperparameters \" + str(rf_grid.best_params_) + \" Best Score: \" + str(rf_grid.best_score_))\n",
    "        res=[ rf_grid.score(X_train,y_train),\n",
    "           rf_grid.score(X_test,y_test),\n",
    "           precision_score(y_test,rf_predict,average ='weighted'),\n",
    "           recall_score(y_test,rf_predict,average ='weighted'),f1_score(y_test,rf_predict,average ='weighted')]\n",
    "        result = pd.DataFrame(np.array(res).reshape(-1,5))\n",
    "        return result\n",
    "\n",
    "    def adaBoost(self,X_train,y_train,X_test,y_test,inp_params):\n",
    "        print(\"AdaBoost Classifier Starting...\")\n",
    "        ab = AdaBoostClassifier(random_state=0)\n",
    "        ab_grid = GridSearchCV(ab, inp_params, verbose=False, cv=3)\n",
    "        ab_grid.fit(X_train,y_train)\n",
    "        ab_predict = ab_grid.predict(X_test)\n",
    "        #self.printaccuracy(y_test,ab_predict,\"AdaBoost\")\n",
    "        #print(\"Best Hyperparameters \" + str(ab_grid.best_params_) + \" Best Score: \" + str(ab_grid.best_score_))\n",
    "        res=[ab_grid.score(X_train,y_train),\n",
    "           ab_grid.score(X_test,y_test),\n",
    "           precision_score(y_test,ab_predict,average ='weighted'),\n",
    "           recall_score(y_test,ab_predict,average ='weighted'),f1_score(y_test,ab_predict,average ='weighted')]\n",
    "        result = pd.DataFrame(np.array(res).reshape(-1,5))\n",
    "        return result\n",
    "    \n",
    "    def gaussianNaiveBaise(self,X_train,y_train,X_test,y_test):\n",
    "        print(\"GaussianNaiveBaive Classifier Starting... \")\n",
    "        gnb = GaussianNB()\n",
    "        gnb.fit(X_train,y_train)\n",
    "        gnb_predict = gnb.predict(X_test)\n",
    "        #self.printaccuracy(y_test,gnb_predict,\"Naive Bayes\")\n",
    "        res = [gnb.score(X_train,y_train),\n",
    "           gnb.score(X_test,y_test),\n",
    "           precision_score(y_test,gnb_predict,average ='weighted'),\n",
    "           recall_score(y_test,gnb_predict,average ='weighted'),f1_score(y_test,gnb_predict,average ='weighted')]\n",
    "        result = pd.DataFrame(np.array(res).reshape(-1,5))\n",
    "        return result\n",
    "\n",
    "    def neuralNetworks(self,X_train,y_train,X_test,y_test,inp_params):\n",
    "        print(\"NeuralNetworks Classifier Starting...\")\n",
    "        nn = MLPClassifier(solver='sgd',random_state=0)\n",
    "        nn_grid = GridSearchCV(nn, inp_params, cv=3)\n",
    "        nn_grid.fit(X_train,y_train)\n",
    "        nn_predict = nn_grid.predict(X_test)\n",
    "        #self.printaccuracy(y_test,nn_predict,\"Neural Networks\")\n",
    "        #print(\"Best Hyperparameters \" + str(nn_grid.best_params_) + \" Best Score: \" + str(nn_grid.best_score_))\n",
    "        res = [nn_grid.score(X_train,y_train),\n",
    "           nn_grid.score(X_test,y_test),\n",
    "           precision_score(y_test,nn_predict,average ='weighted'),\n",
    "           recall_score(y_test,nn_predict,average ='weighted'),f1_score(y_test,nn_predict,average ='weighted')]\n",
    "        result = pd.DataFrame(np.array(res).reshape(-1,5))\n",
    "        return result\n",
    "\n",
    "    def train_models(self,X_train,y_train,X_test,y_test,lr_params,knn_params,svm_params,decisiontree_params,random_forest_params,adaboost_params,nn_params,data):\n",
    "        results = self.knn_grid_search(X_train,y_train,X_test,y_test,knn_params)\n",
    "        results = pd.concat([results,self.svm_model(X_train,y_train,X_test,y_test,svm_params)])\n",
    "        results = pd.concat([results,self.decisionTreeClassifier(X_train,y_train,X_test,y_test,decisiontree_params)])\n",
    "        results = pd.concat([results,self.randomForest(X_train,y_train,X_test,y_test,random_forest_params)])\n",
    "        results = pd.concat([results,self.adaBoost(X_train,y_train,X_test,y_test,adaboost_params)])\n",
    "        results = pd.concat([results,self.logisticRegression(X_train,y_train,X_test,y_test,lr_params)])\n",
    "        results = pd.concat([results,self.gaussianNaiveBaise(X_train,y_train,X_test,y_test)])\n",
    "        results = pd.concat([results,self.neuralNetworks(X_train,y_train,X_test,y_test,nn_params)])\n",
    "        A=[data,data,data,data,data,data,data,data]\n",
    "        B =['Logistic Regression','KNN','SVM','Decision Tree','Random Forest','Adaboost','GaussionNB','Nueral Network']\n",
    "        C=['Train Accuracy','Test Accuracy','Precision','Recall','F1 score']\n",
    "        results.index = [A,B]\n",
    "        results.columns=C\n",
    "        return results\n",
    "\n",
    "    def creditCardDataset(self):\n",
    "        #For credit card Defaulters \n",
    "        df = pd.read_csv(\"../Datasets/Credit_card/credit.csv\")\n",
    "        df.drop(df.columns[0], axis=1, inplace=True)\n",
    "        df.dropna(axis=0, inplace=True)\n",
    "        df = df.iloc[1:]\n",
    "        df = df.astype(float)\n",
    "        df = self.removeoutliers(df,inplace=True)\n",
    "        X = df.iloc[:,:23]\n",
    "        y = df.iloc[:,23:24]\n",
    "        X = self.normalizedata(X)\n",
    "        X_train,X_test,y_train,y_test = self.train_split(X,y)\n",
    "        lr_params = {'C':np.logspace(-4, 4, 20),\n",
    "                    'penalty' :['l1','l2'],\n",
    "                    'max_iter':[10,100,1000]}\n",
    "        knn_params = {'n_neighbors':np.array(range(2,10))}\n",
    "        svm_params = { 'C' : np.logspace(0, 3, 4), 'gamma' : np.logspace(-2, 1, 4)}\n",
    "        decisiontree_params = {'max_depth' : np.arange(1, 10, 10),'min_samples_split': np.arange(0.1, 1.0, 10)}\n",
    "        random_forest_params = {'n_estimators' : np.arange(10,100,10),'max_depth' : np.arange(1,6,2)}\n",
    "        adaBoost_params = {'n_estimators' : np.arange(10,100,10)}\n",
    "        nn_params = {'hidden_layer_sizes': np.arange(30,150,20),'learning_rate': ['constant','invscaling','adaptive'],'max_iter': np.arange(20,200,50)}\n",
    "        creditCardDataset_results = self.train_models(X_train,y_train.values.ravel(),X_test,y_test.values.ravel(),lr_params,knn_params,svm_params,decisiontree_params,random_forest_params, adaBoost_params,nn_params,'Credit_Card')\n",
    "        creditCardDataset_results\n",
    "        return creditCardDataset_results\n",
    "\n",
    "\n",
    "    def australianCredit(self):\n",
    "        df = (pd.read_csv(\"../Datasets/Australian_Credit/australian.dat\", sep='\\s+', header=None))\n",
    "        x = df.iloc[:,:14]\n",
    "        y= df.iloc[:,14:15]\n",
    "        x = (x - x.min())/(x.max() - x.min())\n",
    "        df = pd.concat([x,y],axis=1)\n",
    "        z = np.abs(stats.zscore(df))\n",
    "        df = df[(z < 3).all(axis=1)]\n",
    "        X = df.iloc[:,:14]\n",
    "        y= df.iloc[:,14:15]\n",
    "        X_train,X_test,y_train,y_test = self.train_split(X,y)\n",
    "        lr_params = {'C':np.logspace(-4, 4, 20),\n",
    "                    'penalty' :['l1','l2'],\n",
    "                    'max_iter':[10,100,1000]}\n",
    "        knn_params = {'n_neighbors':np.array(range(2,10))}\n",
    "        svm_params = { 'C' : np.logspace(0, 3, 4), 'gamma' : np.logspace(-2, 1, 4)}\n",
    "        decisiontree_params = {'max_depth' : np.arange(1, 10, 10),'min_samples_split': np.arange(0.1, 1.0, 10)}\n",
    "        random_forest_params = {'n_estimators' : np.arange(10,100,10),'max_depth' : np.arange(1,6,2)}\n",
    "        adaBoost_params = {'n_estimators' : np.arange(10,100,10)}\n",
    "        nn_params = {'hidden_layer_sizes': np.arange(30,150,20),'learning_rate': ['constant','invscaling','adaptive'],'max_iter': np.arange(20,200,50)}\n",
    "        australianCredit_results = self.train_models(X_train,\n",
    "                                                     y_train.values.ravel(),\n",
    "                                                     X_test,y_test.values.ravel(), \n",
    "                                                     lr_params,\n",
    "                                                     knn_params,\n",
    "                                                     svm_params,\n",
    "                                                     decisiontree_params,\n",
    "                                                     random_forest_params, \n",
    "                                                     adaBoost_params,\n",
    "                                                     nn_params,\n",
    "                                                    'Australian_Credit')\n",
    "        australianCredit_results\n",
    "        return australianCredit_results\n",
    "        \n",
    "    def germanCredit(self):\n",
    "        df = (pd.read_csv(\"../Datasets/German_credit_card/german.data-numeric\", sep='\\s+', header=None))\n",
    "        df.dropna(inplace=True)\n",
    "        X = df.iloc[:,:24]\n",
    "        y = df.iloc[:,24:25]\n",
    "        X = (X - X.min())/(X.max() - X.min())     \n",
    "        df = pd.concat([X,y],axis=1)\n",
    "        z = np.abs(stats.zscore(df))\n",
    "        df = df[(z < 3).all(axis=1)]\n",
    "        X = df.iloc[:,:24]\n",
    "        y= df.iloc[:,24:25]\n",
    "        X_train,X_test,y_train,y_test = self.train_split(X,y)\n",
    "        lr_params = {'C':np.logspace(-4, 4, 20),\n",
    "                    'penalty' :['l1','l2'],\n",
    "                    'max_iter':[10,100,1000]}\n",
    "        knn_params = {'n_neighbors':np.array(range(2,10))}\n",
    "        svm_params = { 'C' : np.logspace(0, 3, 4), 'gamma' : np.logspace(-2, 1, 4)}\n",
    "        decisiontree_params = {'max_depth' : np.arange(1, 10, 10),'min_samples_split': np.arange(0.1, 1.0, 10)}\n",
    "        random_forest_params = {'n_estimators' : np.arange(10,100,10),'max_depth' : np.arange(1,6,2)}\n",
    "        adaBoost_params = {'n_estimators' : np.arange(10,100,10)}\n",
    "        nn_params = {'hidden_layer_sizes': np.arange(30,150,20),'learning_rate': ['constant','invscaling','adaptive'],'max_iter': np.arange(20,200,50)}\n",
    "        germanCredit_results = self.train_models(X_train,y_train.values.ravel(),X_test,y_test.values.ravel(), lr_params,svm_params,decisiontree_params,random_forest_params, adaBoost_params,nn_params,'German_Credit')\n",
    "        germanCredit\n",
    "        return germanCredit\n",
    "\n",
    "    def thoratic(self):\n",
    "        data = pd.read_csv(\"../Datasets/9.ThoraticSurgeryData/ThoraricSurgery.arff\",delimiter = ',',names=[\"DGN\", \"PRE4\", \"PRE5\", \"PRE6\",\"PRE7\",\"PRE8\",\"PRE9\",\"PRE10\",\"PRE11\",\"PRE14\",\"PRE17\",\"PRE19\",\"PRE25\",\"PRE30\",\"PRE32\",\"AGE\",\"Risk1Y\"])\n",
    "        data.head()\n",
    "        #Preprocessing\n",
    "        X = pd.DataFrame(data,columns=[\"DGN\", \"PRE4\", \"PRE5\", \"PRE6\",\"PRE7\",\"PRE8\",\"PRE9\",\"PRE10\",\"PRE11\",\"PRE14\",\"PRE17\",\"PRE19\",\"PRE25\",\"PRE30\",\"PRE32\",\"AGE\"])\n",
    "        cat = [\"DGN\",\"PRE6\",\"PRE7\",\"PRE8\",\"PRE9\",\"PRE10\",\"PRE11\",\"PRE14\",\"PRE17\",\"PRE19\",\"PRE25\",\"PRE30\",\"PRE32\"]\n",
    "        for i in cat:\n",
    "            X[i] = pd.Categorical(X[i]).codes\n",
    "        y = data.iloc[:,16:17]\n",
    "        y['Risk1Y'] = pd.Categorical(y['Risk1Y']).codes\n",
    "        X = self.normalizedata(X)\n",
    "        X_train,X_test,y_train,y_test = self.train_split(X,y)\n",
    "        lr_params = {'C':np.logspace(-4, 4, 20),\n",
    "                    'penalty' :['l1','l2'],\n",
    "                    'max_iter':[1000]}\n",
    "        knn_params={'n_neighbors':np.array(range(2,50))}\n",
    "        svm_params = { 'C' : np.logspace(0, 3, 4), 'gamma' : np.logspace(-2, 1, 4)}\n",
    "        decisiontree_params = { 'max_depth' : np.arange(5,10,1)}\n",
    "        random_forest_params = { 'n_estimators' : np.arange(10,100,10), 'max_depth' : np.arange(5,50,5)}\n",
    "        adaBoost_params = { 'n_estimators' : np.arange(10,100,10)}\n",
    "        nn_params = {\n",
    "                    'hidden_layer_sizes': np.arange(50,150,20),\n",
    "                    'learning_rate': ['constant','adaptive'],\n",
    "                    'max_iter': np.arange(200,300,50)\n",
    "                    }\n",
    "        Thoratic_results = self.train_models(X_train,y_train.values.ravel(),X_test,y_test.values.ravel(),lr_params,knn_params,svm_params,decisiontree_params,random_forest_params, adaBoost_params,nn_params,'Thoracic Surgery Data')\n",
    "        return Thoratic_results \n",
    "        \n",
    "    def seismicbumps(self):\n",
    "        data = pd.read_csv(\"../Datasets/SeismicBumps/seismic-bumps.arff\",delimiter = ',',names=[\"seismic\",\"seismoacoustic\",\"shift\",\"genergy\",\"gpuls\",\"gdenergy\",\"gdpuls\",\"ghazard\",\"nbumps\",\"nbumps2\",\"nbumps3\",\"nbumps4\",\"nbumps5\",\"nbumps6\",\"nbumps7\",\"nbumps89\",\"energy\",\"maxenergy\",\"class\"])\n",
    "        data.head()\n",
    "        #Preprocessing\n",
    "        X = pd.DataFrame(data,columns=[\"seismic\",\"seismoacoustic\",\"shift\",\"genergy\",\"gpuls\",\"gdenergy\",\"gdpuls\",\"ghazard\",\"nbumps\",\"nbumps2\",\"nbumps3\",\"nbumps4\",\"nbumps5\",\"nbumps6\",\"nbumps7\",\"nbumps89\",\"energy\",\"maxenergy\"])\n",
    "        cat = [\"seismic\",\"seismoacoustic\",\"shift\",\"ghazard\"]\n",
    "        for i in cat:\n",
    "            X[i] = pd.Categorical(X[i]).codes\n",
    "        y = data.iloc[:,18:29]\n",
    "        y['class'] = pd.Categorical(y['class']).codes\n",
    "        X = self.normalizedata(X)\n",
    "        X_train,X_test,y_train,y_test = self.train_split(X,y)\n",
    "        lr_params = {'C':np.logspace(-4, 4, 20),\n",
    "                    'penalty' :['l1','l2'],\n",
    "                    'max_iter':[1000]}\n",
    "        knn_params={'n_neighbors':np.array(range(2,50))}\n",
    "        svm_params = { 'C' : np.logspace(0, 3, 4), 'gamma' : np.logspace(-2, 1, 4)}\n",
    "        decisiontree_params = { 'max_depth' : np.arange(5,10,1)}\n",
    "        random_forest_params = { 'n_estimators' : np.arange(10,100,10), 'max_depth' : np.arange(5,50,5)}\n",
    "        adaBoost_params = { 'n_estimators' : np.arange(10,100,10)}\n",
    "        nn_params = {\n",
    "                    'hidden_layer_sizes': np.arange(50,150,20),\n",
    "                    'learning_rate': ['constant','adaptive'],\n",
    "                    'max_iter': np.arange(200,300,50)\n",
    "                    }\n",
    "\n",
    "        Seismic_results = self.train_models(X_train,y_train.values.ravel(),X_test,y_test.values.ravel(),lr_params,knn_params,svm_params,decisiontree_params,random_forest_params, adaBoost_params,nn_params,\n",
    "                                                    'Seismic-Bumps')\n",
    "        return Seismic_results\n",
    "        \n",
    "    def steel_plates_faults(self):\n",
    "        #Multiclass\n",
    "        long_list= ['X_Minimum', 'X_Maximum', 'Y_Minimum', 'Y_Maximum', 'Pixels_Areas', 'X_Perimeter', 'Y_Perimeter', 'Sum_of_Luminosity', 'Minimum_of_Luminosity', 'Maximum_of_Luminosity', 'Length_of_Conveyer', 'TypeOfSteel_A300', 'TypeOfSteel_A400', 'Steel_Plate_Thickness', 'Edges_Index', 'Empty_Index', 'Square_Index', 'Outside_X_Index', 'Edges_X_Index', 'Edges_Y_Index', 'Outside_Global_Index', 'LogOfAreas', 'Log_X_Index', 'Log_Y_Index', 'Orientation_Index', 'Luminosity_Index', 'SigmoidOfAreas', 'Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']\n",
    "        data = pd.read_csv(\"../Datasets/SteelPlatesFaults/Faults.NNA\",delimiter = '\\s+',names=long_list)\n",
    "        X = pd.DataFrame(data,columns=['X_Minimum', 'X_Maximum', 'Y_Minimum', 'Y_Maximum', 'Pixels_Areas', 'X_Perimeter', 'Y_Perimeter', 'Sum_of_Luminosity', 'Minimum_of_Luminosity', 'Maximum_of_Luminosity', 'Length_of_Conveyer', 'TypeOfSteel_A300', 'TypeOfSteel_A400', 'Steel_Plate_Thickness', 'Edges_Index', 'Empty_Index', 'Square_Index', 'Outside_X_Index', 'Edges_X_Index', 'Edges_Y_Index', 'Outside_Global_Index', 'LogOfAreas', 'Log_X_Index', 'Log_Y_Index', 'Orientation_Index', 'Luminosity_Index', 'SigmoidOfAreas'])\n",
    "        y = data.iloc[:,27:34]\n",
    "        #Converting 7 columns into one y 'class' column\n",
    "        def fun1(x):\n",
    "            for i in range(len(x)):\n",
    "                if x[i] == 1:\n",
    "                    return i\n",
    "        y1= []        \n",
    "        for j in range(len(y)):        \n",
    "            y1.append((fun1(y.iloc[j]))) \n",
    "        y2 = pd.DataFrame(y1)\n",
    "        y2.columns=['Class']\n",
    "        y=y2\n",
    "        X = self.normalizedata(X)\n",
    "        X_train,X_test,y_train,y_test = self.train_split(X,y)\n",
    "        lr_params = {'C':np.logspace(-4, 4, 20),\n",
    "                    'penalty' :['l1','l2'],\n",
    "                    'max_iter':[1000]}\n",
    "        knn_params={'n_neighbors':np.array(range(2,50))}\n",
    "        svm_params = { 'C' : np.logspace(0, 3, 4), 'gamma' : np.logspace(-2, 1, 4)}\n",
    "        decisiontree_params = { 'max_depth' : np.arange(5,10,1)}\n",
    "        random_forest_params = { 'n_estimators' : np.arange(10,100,10), 'max_depth' : np.arange(5,50,5)}\n",
    "        adaBoost_params = { 'n_estimators' : np.arange(10,100,10)}\n",
    "        nn_params = {\n",
    "                    'hidden_layer_sizes': np.arange(50,150,20),\n",
    "                    'learning_rate': ['constant','adaptive'],\n",
    "                    'max_iter': np.arange(200,300,50)\n",
    "                    }\n",
    "\n",
    "        Steel_Plates_Faults_results = self.train_models(X_train,y_train.values.ravel(),X_test,y_test.values.ravel(),lr_params,knn_params,svm_params,decisiontree_params,random_forest_params, adaBoost_params,nn_params,\n",
    "                                                    'Steel_Plates_Faults')\n",
    "        return Steel_Plates_Faults_results\n",
    "        \n",
    "    def diabetic_retinopaty(self):\n",
    "        col = ['Quality','Pre-screening','MA1','MA2','MA3','MA4','MA5','MA6','MA7',\n",
    "              'exudates1','exudates2','exudates3','exudates4','exudates5','exudates6','exudates7',\n",
    "              'macula_opticdisc','opticdisc_diamter','AM/FM','Class_label']\n",
    "        data = pd.read_csv('messidor_features.arff',error_bad_lines=False)\n",
    "        data.columns = col\n",
    "        data = data[data['Quality'] != 0]\n",
    "        data.drop(['Quality','AM/FM','Pre-screening','MA1','MA2','MA3','MA5','MA6','exudates3','exudates4'\n",
    "               ,'exudates6','exudates7'],axis=1,inplace=True)\n",
    "        data = self.removeoutliers(data,inplace=True)\n",
    "        X = data.drop('Class_label',axis=1)\n",
    "        X_copy = X.copy()\n",
    "        y= data['Class_label']\n",
    "        X = self.normalizedata(X)\n",
    "        X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=0)\n",
    "        lr_params = {'C':np.logspace(-4, 4, 20),\n",
    "                    'penalty' :['l1','l2'],\n",
    "                    'max_iter':[1000]}\n",
    "        knn_params={'n_neighbors':np.array(range(2,50))}\n",
    "        svm_params = { 'C' : np.logspace(0, 3, 4), 'gamma' : np.logspace(-2, 1, 4)}\n",
    "        decisiontree_params = { 'max_depth' : np.arange(5,10,1)}\n",
    "        random_forest_params = { 'n_estimators' : np.arange(10,100,10), 'max_depth' : np.arange(5,10,1)}\n",
    "        adaBoost_params = { 'n_estimators' : np.arange(10,100,10)}\n",
    "        nn_params = {\n",
    "                    'hidden_layer_sizes': np.arange(50,150,20),\n",
    "                    'learning_rate': ['constant','adaptive'],\n",
    "                    'max_iter': np.arange(200,300,50)\n",
    "                    }\n",
    "        diabetic_retinopaty_results = self.train_models(X_train,\n",
    "                                                     y_train,\n",
    "                                                     X_test,y_test, \n",
    "                                                     lr_params,\n",
    "                                                     knn_params,\n",
    "                                                     svm_params,\n",
    "                                                     decisiontree_params,\n",
    "                                                     random_forest_params, \n",
    "                                                     adaBoost_params,\n",
    "                                                     nn_params,\n",
    "                                                    'Diabetic Retinopathy')\n",
    "        diabetic_retinopaty_results\n",
    "        return diabetic_retinopaty_results\n",
    "    \n",
    "    def Breast_Cancer_Wisconsin(self):\n",
    "        col = ['ID Number','Diagnosis','radius','texture','perimeter','area','smoothness','compactness','concavity','concave points','symmetry','fractal dimension'\n",
    "        ,'radius2','texture2','perimeter2','area2','smoothness2','compactness2','concavity2','concave points2','symmetry2','fractal dimension2'\n",
    "        ,'radius3','texture3','perimeter3','area3','smoothness3','compactness3','concavity3','concave points3','symmetry3','fractal dimension3']\n",
    "\n",
    "        data = pd.read_csv('wdbc.data',error_bad_lines=False)\n",
    "        data.columns = col\n",
    "        data.drop('ID Number',axis=1,inplace=True)\n",
    "        data['Diagnosis'] = pd.Categorical(data['Diagnosis']).codes\n",
    "        data = self.removeoutliers(data,inplace=True)\n",
    "        X = data.drop('Diagnosis',axis=1)\n",
    "        X_copy = X.copy()\n",
    "        y= data['Diagnosis']\n",
    "        X = self.normalizedata(X)\n",
    "        X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=0)\n",
    "        lr_params = {'C':np.logspace(-4, 4, 20),\n",
    "                    'penalty' :['l1','l2'],\n",
    "                    'max_iter':[1000]}\n",
    "        knn_params={'n_neighbors':np.array(range(2,50))}\n",
    "        svm_params = { 'C' : np.logspace(0, 3, 4), 'gamma' : np.logspace(-2, 1, 4)}\n",
    "        decisiontree_params = { 'max_depth' : np.arange(5,10,1)}\n",
    "        random_forest_params = { 'n_estimators' : np.arange(10,100,10), 'max_depth' : np.arange(5,50,5)}\n",
    "        adaBoost_params = { 'n_estimators' : np.arange(10,100,10)}\n",
    "        nn_params = {\n",
    "                    'hidden_layer_sizes': np.arange(50,150,20),\n",
    "                    'learning_rate': ['constant','adaptive'],\n",
    "                    'max_iter': np.arange(200,300,50)\n",
    "                    }\n",
    "        Breast_Cancer_Wisconsin_results = self.train_models(X_train,\n",
    "                                                     y_train,\n",
    "                                                     X_test,y_test, \n",
    "                                                     lr_params,\n",
    "                                                     knn_params,\n",
    "                                                     svm_params,\n",
    "                                                     decisiontree_params,\n",
    "                                                     random_forest_params, \n",
    "                                                     adaBoost_params,\n",
    "                                                     nn_params,\n",
    "                                                    'Breast_Cancer_Wisconsin')\n",
    "        return Breast_Cancer_Wisconsin_results\n",
    "    \n",
    "    \n",
    "    def Adults_Salary(self):\n",
    "        col = ['age','workclass','fnlwgt','education','education-num','marital-status','occupation','relationship','race','sex',\n",
    "           'capital-gain','capital-loss','hours-per-week','native-country','salary']\n",
    "        data = pd.read_csv(\"../Datasets/Adult_Salary_Data/adult.data\")\n",
    "        data.columns=col\n",
    "        categorical_columns = ['workclass','education','marital-status','occupation','relationship','race','sex','native-country','salary']\n",
    "        for i in categorical_columns:\n",
    "            data[i] = pd.Categorical(data[i]).codes\n",
    "        data.columns = data.columns.str.lstrip()\n",
    "        a = StandardScaler()\n",
    "        X = data.drop(['salary'],axis=1)\n",
    "        X = a.fit_transform(X)\n",
    "        y = data['salary']\n",
    "        X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=0,test_size=0.3)\n",
    "        lr_params = {'C':np.logspace(-4, 4, 20),\n",
    "                        'penalty' :['l1','l2'],\n",
    "                        'max_iter':[1000]}\n",
    "        knn_params={'n_neighbors':np.array(range(2,5))}\n",
    "        svm_params = { 'C' : np.logspace(0, 3, 4), 'gamma' : np.logspace(-2, 1, 4)}\n",
    "        decisiontree_params = { 'max_depth' : np.arange(5,10,1)}\n",
    "        random_forest_params = { 'n_estimators' : np.arange(10,100,10), 'max_depth' : np.arange(5,50,5)}\n",
    "        adaBoost_params = { 'n_estimators' : np.arange(10,100,10)}\n",
    "        nn_params = {\n",
    "                    'hidden_layer_sizes': np.arange(50,150,20),\n",
    "                    'learning_rate': ['constant','adaptive'],\n",
    "                    'max_iter': np.arange(200,300,50)\n",
    "                    }\n",
    "        adult_salary_results = self.train_models(X_train,\n",
    "                                                     y_train,\n",
    "                                                     X_test,y_test, \n",
    "                                                     lr_params,\n",
    "                                                     knn_params,\n",
    "                                                     svm_params,\n",
    "                                                     decisiontree_params,\n",
    "                                                     random_forest_params, \n",
    "                                                     adaBoost_params,\n",
    "                                                     nn_params,\n",
    "                                                    'Adult_Salary')\n",
    "        return adult_salary_results\n",
    "    \n",
    "    \n",
    "    def Yeast_Category(self):\n",
    "        col = ['Sequence Name','mcg','gvh','alm','mit','erl','pox','vac','nuc','localization-site']\n",
    "        data = pd.read_csv(\"../Datasets/Yeast_data/yeast.data\",delim_whitespace=True)\n",
    "        data.columns=col\n",
    "        categorical_columns = ['Sequence Name','localization-site']\n",
    "        for i in categorical_columns:\n",
    "            data[i] = pd.Categorical(data[i]).codes\n",
    "        data.columns = data.columns.str.lstrip()\n",
    "        a = StandardScaler()\n",
    "        X = data.drop(['localization-site'],axis=1)\n",
    "        X = a.fit_transform(X)\n",
    "        y = data['localization-site']\n",
    "        X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=0,test_size=0.3)\n",
    "        lr_params = {'C':np.logspace(-4, 4, 20),\n",
    "                            'penalty' :['l1','l2'],\n",
    "                            'max_iter':[1000]}\n",
    "        knn_params={'n_neighbors':np.array(range(2,5))}\n",
    "        svm_params = { 'C' : np.logspace(0, 3, 4), 'gamma' : np.logspace(-2, 1, 4)}\n",
    "        decisiontree_params = { 'max_depth' : np.arange(5,10,1)}\n",
    "        random_forest_params = { 'n_estimators' : np.arange(10,100,10), 'max_depth' : np.arange(5,50,5)}\n",
    "        adaBoost_params = { 'n_estimators' : np.arange(10,100,10)}\n",
    "        nn_params = {\n",
    "                    'hidden_layer_sizes': np.arange(50,150,20),\n",
    "                    'learning_rate': ['constant','adaptive'],\n",
    "                    'max_iter': np.arange(200,300,50)\n",
    "                    }\n",
    "        yeast_category_results = self.train_models(X_train,\n",
    "                                                     y_train,\n",
    "                                                     X_test,y_test, \n",
    "                                                     lr_params,\n",
    "                                                     knn_params,\n",
    "                                                     svm_params,\n",
    "                                                     decisiontree_params,\n",
    "                                                     random_forest_params, \n",
    "                                                     adaBoost_params,\n",
    "                                                     nn_params,\n",
    "                                                    'Yeast_Category')\n",
    "        return yeast_category_results\n",
    "    \n",
    "    \n",
    "    \n",
    "    def getbest(self,data,name):\n",
    "        temp = data.xs(name) \n",
    "        temp = temp[temp['F1 score'] == temp['F1 score'].max()]\n",
    "        temp.index=[name+' - '+temp.index[0]]\n",
    "        return temp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knn Grid Search Starting...\n",
      "SVM Classification Starting...\n"
     ]
    }
   ],
   "source": [
    "entrypoint = entryPoint()\n",
    "\n",
    "#GIRISH\n",
    "# temp = entrypoint.diabetic_retinopaty()\n",
    "# best_report = entrypoint.getbest(temp,'Diabetic Retinopathy')\n",
    "# Final_report = temp\n",
    "# temp = entrypoint.Breast_Cancer_Wisconsin()\n",
    "# best_report = pd.concat([best_report,entrypoint.getbest(temp,'Breast_Cancer_Wisconsin')])\n",
    "# Final_report = pd.concat([Final_report,temp])\n",
    "\n",
    "\n",
    "# ##Gursimran SIngh\n",
    "# temp = entrypoint.thoratic()\n",
    "# best_report = pd.concat([best_report,entrypoint.getbest(temp,'Thoracic Surgery Data')])\n",
    "# Final_report = pd.concat([Final_report,temp])\n",
    "\n",
    "# temp = entrypoint.seismicbumps()\n",
    "# best_report = pd.concat([best_report,entrypoint.getbest(temp,'Seismic-Bumps')])\n",
    "# Final_report = pd.concat([Final_report,temp])\n",
    "\n",
    "# temp = entrypoint.steel_plates_faults()\n",
    "# best_report = pd.concat([best_report,entrypoint.getbest(temp,'Steel_Plates_Faults')])\n",
    "# Final_report = pd.concat([Final_report,temp])\n",
    "\n",
    "# ##Aravind\n",
    "temp = entrypoint.Adults_Salary()\n",
    "best_report = pd.concat([best_report,entrypoint.getbest(temp,'Adult_Salary')])\n",
    "Final_report = temp\n",
    "\n",
    "temp = entrypoint.Yeast_Category()\n",
    "best_report = pd.concat([best_report,entrypoint.getbest(temp,'Yeast_Category')])\n",
    "Final_report = pd.concat([Final_report,temp])\n",
    "\n",
    "# ##Darshan\n",
    "# temp = entrypoint.creditCardDataset()\n",
    "# best_report = pd.concat([best_report,entrypoint.getbest(temp,'Credit_Card')])\n",
    "# Final_report = pd.concat([Final_report,temp])\n",
    "\n",
    "# temp = entrypoint.australianCredit()\n",
    "# best_report = pd.concat([best_report,entryplloint.getbest(temp,'Australia_Credit')])\n",
    "# Final_report = pd.concat([Final_report,temp])\n",
    "\n",
    "# temp = entrypoint.germanCredit()\n",
    "# best_report = pd.concat([best_report,entrypoint.getbest(temp,'German_Credit')])\n",
    "# Final_report = pd.concat([Final_report,temp])\n",
    "\n",
    "\n",
    "print('Best Report')\n",
    "best_report\n",
    "print(\"  \")\n",
    "print('Final Report')\n",
    "Final_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
