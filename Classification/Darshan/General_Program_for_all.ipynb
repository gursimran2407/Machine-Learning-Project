{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M672TMuWxHmV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "class entryPoint():\n",
    "\n",
    "    def printaccuracy(self,y_test,predict,model):\n",
    "        print(model,\" report\")\n",
    "        print(\"-------------------------------------\")\n",
    "        print(\" \")\n",
    "        print(\" Confusion Matrix \" ,confusion_matrix(y_test,predict))\n",
    "        print(classification_report(y_test,predict))\n",
    "        print(\" \")\n",
    "        print(\"-------------------------------------\")\n",
    "        print(\" \")\n",
    "    \n",
    "    def normalizedata(self,X):\n",
    "        SS = StandardScaler()\n",
    "        X = SS.fit_transform(X)\n",
    "        print(\"Normalization done\")\n",
    "        return X\n",
    "\n",
    "    def removeoutliers(self,data,inplace=False):\n",
    "        prev_rows = len(data)\n",
    "        data_copy = data.copy()\n",
    "        z_score = np.abs(stats.zscore(data_copy))\n",
    "        data_copy = data_copy[(z_score < 3).all(axis=1)]\n",
    "        if inplace:\n",
    "          data=data_copy\n",
    "        print(\"Before removing outliers , rows - \", prev_rows)\n",
    "        print(\"After removing outliers , rows -\", len(data_copy))\n",
    "        print(\"Number of records deleted - \", (prev_rows - len(data_copy)))\n",
    "        return data_copy\n",
    "\n",
    "    def train_split(self,X,y,test_size=0.2,random_state=0):\n",
    "        X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)\n",
    "        return X_train,X_test,y_train,y_test\n",
    "\n",
    "    def knn(self,X_train,y_train,X_test,y_test):\n",
    "        print(\"Knn\")\n",
    "        knn_error = []\n",
    "        for i in range(2,10):\n",
    "          knn = KNeighborsClassifier(n_neighbors=i)\n",
    "          knn.fit(X_train,y_train)\n",
    "          knn_predict= knn.predict(X_test)\n",
    "          print(type(knn_predict))\n",
    "          print(type(y_test))\n",
    "          knn_error.append(np.mean(y_test!=knn_predict))\n",
    "        plt.plot(range(2,50),knn_error)\n",
    "        plt.xlabel(\"K value\")\n",
    "        plt.ylabel(\"Error\")\n",
    "    \n",
    "    def knn_grid_search(self,X_train,y_train,X_test,y_test):\n",
    "        print(\"Knn Grid Search Starting...\")\n",
    "        neighbors={'n_neighbors':np.array(range(2,10))}\n",
    "        knn_grid=GridSearchCV(KNeighborsClassifier(),neighbors,verbose=False,refit=True,cv=3)\n",
    "        knn_grid.fit(X_train,y_train.values.ravel())\n",
    "        knn_predict = knn_grid.predict(X_test)\n",
    "        self.printaccuracy(y_test,knn_predict,\"KNN\")\n",
    "        print(\"Best Hyperparameters \" + str(knn_grid.best_params_) + \" Best Score: \" + str(knn_grid.best_score_))\n",
    "        flScore = f1_score(y_test,knn_predict)\n",
    "        return flScore\n",
    "    \n",
    "    def logisticRegression(self,X_train,y_train,X_test,y_test):\n",
    "        print(\"Logistic Regression classification Starting...\")\n",
    "        Co_reg= np.logspace(-4, 4, 20)\n",
    "        penalty_reg = ['l1','l2']\n",
    "        max_iteration = [10,100,1000]\n",
    "        score = []\n",
    "        for pen in penalty_reg:\n",
    "            for i in Co_reg:\n",
    "                for it in max_iteration:\n",
    "                    clf = LogisticRegression(random_state=0, solver='liblinear', penalty=pen , C=i, max_iter=it).fit(X_train, y_train.values.ravel())\n",
    "                    score.append(clf.score(X_test, y_test.values.ravel()))\n",
    "\n",
    "        print(\"Best Score : \" + str(max(score)))\n",
    "        \n",
    "    def svm_model(self,X_train,y_train,X_test,y_test,inp_params):\n",
    "        print(\"SVM Classification Starting...\")\n",
    "        svm = SVC(kernel='rbf',random_state=0)\t\n",
    "        params = inp_params\n",
    "        svm_grid = GridSearchCV(svm, params, verbose=1, cv=3,return_train_score=True)\n",
    "        svm_grid.fit(X_train,y_train.ravel())\n",
    "        svm_predict = svm_grid.predict(X_test)\n",
    "        self.printaccuracy(y_test,svm_predict,\"SVM\")\n",
    "        print(\"Best Hyperparameters \" + str(svm_grid.best_params_) + \" Best Score: \" + str(svm_grid.best_score_))\n",
    "        return f1_score(y_test,svm_predict)\n",
    "\n",
    "    def decisionTreeClassifier(self,X_train,y_train,X_test,y_test,inp_params):\n",
    "        print(\"Decisiontree Classifier Starting...\")\n",
    "        params = inp_params\n",
    "        decisionTree_grid = GridSearchCV(DecisionTreeClassifier(), params, verbose=1, cv=3,return_train_score=True)\n",
    "        decisionTree_grid.fit(X_train,y_train.ravel())\n",
    "        decisionTree_predict = decisionTree_grid.predict(X_test)\n",
    "        self.printaccuracy(y_test,decisionTree_predict,\"DecisionTree\")\n",
    "        print(\"Best Hyperparameters \" + str(decisionTree_predict.best_params_) + \" Best Score: \" + str(decisionTree_predict.best_score_))\n",
    "        return f1_score(y_test,decisionTree_predict)\n",
    "    \n",
    "    def randomForest(self,X_train,y_train,X_test,y_test,inp_params):\n",
    "        print(\"randomForest Classifier Starting...\")\n",
    "        rf = RandomForestClassifier()\n",
    "        params = inp_params\n",
    "        rf_grid = GridSearchCV(rf, params, verbose=1, cv=3)\n",
    "        rf_grid.fit(X_train,y_train.ravel())\n",
    "        rf_predict = rf_grid.predict(X_test)\n",
    "        self.printaccuracy(y_test,rf_predict,\"RandomForest\")\n",
    "        print(\"Best Hyperparameters \" + str(rf_grid.best_params_) + \" Best Score: \" + str(rf_grid.best_score_))\n",
    "        return f1_score(y_test,rf_predict)\n",
    "\n",
    "    def adaBoost(self,X_train,y_train,X_test,y_test,inp_params):\n",
    "        print(\"AdaBoost Classifier Starting...\")\n",
    "        ab = AdaBoostClassifier()\n",
    "        params = inp_params\n",
    "        ab_grid = GridSearchCV(ab, params, verbose=1, cv=3)\n",
    "        ab_grid.fit(X_train,y_train)\n",
    "        ab_predict = ab_grid.predict(X_test)\n",
    "        self.printaccuracy(y_test,ab_predict,\"AdaBoost\")\n",
    "        print(\"Best Hyperparameters \" + str(ab_grid.best_params_) + \" Best Score: \" + str(ab_grid.best_score_))\n",
    "        return f1_score(y_test,ab_predict)\n",
    "    \n",
    "    def gaussianNaiveBaise(self,X_train,y_train,X_test,y_test):\n",
    "        print(\"GaussianNaiveBaive Classifier Starting... \")\n",
    "        gnb = GaussianNB()\n",
    "        gnb.fit(X_train,y_train)\n",
    "        gnb_predict = gnb.predict(X_test)\n",
    "        self.printaccuracy(y_test,gnb_predict,\"Naive Bayes\")\n",
    "        return f1_score(y_test,gnb_predict)\n",
    "\n",
    "    def neuralNetworks(self,X_train,y_train,X_test,y_test,inp_params):\n",
    "        print(\"NeuralNetworks Classifier Starting...\")\n",
    "        nn = MLPClassifier(solver='sgd',random_state=0)\n",
    "        params = inp_params\n",
    "        nn_grid = GridSearchCV(nn, params, cv=3)\n",
    "        nn_grid.fit(X_train,y_train)\n",
    "        nn_predict = nn_grid.predict(X_test)\n",
    "        self.printaccuracy(y_test,nn_predict,\"Neural Networks\")\n",
    "        print(\"Best Hyperparameters \" + str(nn_grid.best_params_) + \" Best Score: \" + str(nn_grid.best_score_))\n",
    "        return f1_score(y_test,nn_predict)\n",
    "\n",
    "    def train_models(self,X_train,y_train,X_test,y_test,):\n",
    "        f1scores = []\n",
    "        f1scores.append(1)\n",
    "        f1scores.append(self.knn_grid_search(X_train,y_train,X_test,y_test))\n",
    "        f1scores.append(self.logisticRegression(X_train,y_train,X_test,y_test))\n",
    "        f1scores.append(self.randomForest(X_train,y_train,X_test,y_test))\n",
    "        f1scores.append(self.decisionTreeClassifier(X_train,y_train,X_test,y_test))\n",
    "        f1scores.append(self.adaBoost(X_train,y_train,X_test,y_test))\n",
    "        f1scores.append(self.gaussianNaiveBaise(X_train,y_train,X_test,y_test))\n",
    "        f1scores.append(self.neuralNetworks(X_train,y_train,X_test,y_test))\n",
    "        f1scores.append(self.neuralNetworks(X_train,y_train,X_test,y_test))\n",
    "        f1scores.append(self.randomForest(X_train,y_train,X_test,y_test))\n",
    "        f1scores.append(self.svm_model(X_train,y_train,X_test,y_test))     \n",
    "        return f1scores\n",
    "\n",
    "    def creditCardDataset(self):\n",
    "        #For credit card Defaulters \n",
    "        df = pd.read_csv(\"../Datasets/Credit_card/credit.csv\")\n",
    "        df.drop(df.columns[0], axis=1, inplace=True)\n",
    "        df.dropna(axis=0, inplace=True)\n",
    "        df = df.iloc[1:]\n",
    "        df = df.astype(float)\n",
    "        df = self.removeoutliers(df,inplace=True)\n",
    "        X = df.iloc[:,:23]\n",
    "        y = df.iloc[:,23:24]\n",
    "        X = self.normalizedata(X)\n",
    "        X_train,X_test,y_train,y_test = self.train_split(X,y)\n",
    "        print(X_train.shape)\n",
    "        print(X_test.shape)\n",
    "        print(y_train.shape)\n",
    "        print(y_test.shape)\n",
    "        flscores = self.train_models(X_train,y_train,X_test,y_test)\n",
    "        print(flscores)\n",
    "\n",
    "\n",
    "    def thoratic(self):\n",
    "        data = pd.read_csv(\"../Datasets/9.ThoraticSurgeryData/ThoraricSurgery.arff\",delimiter = ',',names=[\"DGN\", \"PRE4\", \"PRE5\", \"PRE6\",\"PRE7\",\"PRE8\",\"PRE9\",\"PRE10\",\"PRE11\",\"PRE14\",\"PRE17\",\"PRE19\",\"PRE25\",\"PRE30\",\"PRE32\",\"AGE\",\"Risk1Y\"])\n",
    "        data.head()\n",
    "        #Preprocessing\n",
    "        X = pd.DataFrame(data,columns=[\"DGN\", \"PRE4\", \"PRE5\", \"PRE6\",\"PRE7\",\"PRE8\",\"PRE9\",\"PRE10\",\"PRE11\",\"PRE14\",\"PRE17\",\"PRE19\",\"PRE25\",\"PRE30\",\"PRE32\",\"AGE\"])\n",
    "        cat = [\"DGN\",\"PRE6\",\"PRE7\",\"PRE8\",\"PRE9\",\"PRE10\",\"PRE11\",\"PRE14\",\"PRE17\",\"PRE19\",\"PRE25\",\"PRE30\",\"PRE32\"]\n",
    "        for i in cat:\n",
    "            X[i] = pd.Categorical(X[i]).codes\n",
    "        y = data.iloc[:,16:17]\n",
    "        y['Risk1Y'] = pd.Categorical(y['Risk1Y']).codes\n",
    "        X = self.normalizedata(X)\n",
    "        X_train,X_test,y_train,y_test = self.train_split(X,y)\n",
    "        print(X_train.shape)\n",
    "        print(X_test.shape)\n",
    "        print(y_train.shape)\n",
    "        print(y_test.shape)\n",
    "        flscores = self.train_models(X_train,y_train,X_test,y_test)\n",
    "        print(flscores)\n",
    "        \n",
    "    def seismicbumps(self):\n",
    "        data = pd.read_csv(\"../Datasets/SeismicBumps/seismic-bumps.arff\",delimiter = ',',names=[\"seismic\",\"seismoacoustic\",\"shift\",\"genergy\",\"gpuls\",\"gdenergy\",\"gdpuls\",\"ghazard\",\"nbumps\",\"nbumps2\",\"nbumps3\",\"nbumps4\",\"nbumps5\",\"nbumps6\",\"nbumps7\",\"nbumps89\",\"energy\",\"maxenergy\",\"class\"])\n",
    "        data.head()\n",
    "        #Preprocessing\n",
    "        X = pd.DataFrame(data,columns=[\"seismic\",\"seismoacoustic\",\"shift\",\"genergy\",\"gpuls\",\"gdenergy\",\"gdpuls\",\"ghazard\",\"nbumps\",\"nbumps2\",\"nbumps3\",\"nbumps4\",\"nbumps5\",\"nbumps6\",\"nbumps7\",\"nbumps89\",\"energy\",\"maxenergy\"])\n",
    "        cat = [\"seismic\",\"seismoacoustic\",\"shift\",\"ghazard\"]\n",
    "        for i in cat:\n",
    "            X[i] = pd.Categorical(X[i]).codes\n",
    "        y = data.iloc[:,18:29]\n",
    "        y['class'] = pd.Categorical(y['class']).codes\n",
    "        X = self.normalizedata(X)\n",
    "        X_train,X_test,y_train,y_test = self.train_split(X,y)\n",
    "        print(X_train.shape)\n",
    "        print(X_test.shape)\n",
    "        print(y_train.shape)\n",
    "        print(y_test.shape)\n",
    "        flscores = self.train_models(X_train,y_train,X_test,y_test)\n",
    "        print(flscores)\n",
    "        \n",
    "    def steel_plates_faults(self):\n",
    "        #Multiclass\n",
    "        long_list= ['X_Minimum', 'X_Maximum', 'Y_Minimum', 'Y_Maximum', 'Pixels_Areas', 'X_Perimeter', 'Y_Perimeter', 'Sum_of_Luminosity', 'Minimum_of_Luminosity', 'Maximum_of_Luminosity', 'Length_of_Conveyer', 'TypeOfSteel_A300', 'TypeOfSteel_A400', 'Steel_Plate_Thickness', 'Edges_Index', 'Empty_Index', 'Square_Index', 'Outside_X_Index', 'Edges_X_Index', 'Edges_Y_Index', 'Outside_Global_Index', 'LogOfAreas', 'Log_X_Index', 'Log_Y_Index', 'Orientation_Index', 'Luminosity_Index', 'SigmoidOfAreas', 'Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']\n",
    "        data = pd.read_csv(\"../Datasets/SteelPlatesFaults/Faults.NNA\",delimiter = '\\s+',names=long_list)\n",
    "        X = pd.DataFrame(data,columns=['X_Minimum', 'X_Maximum', 'Y_Minimum', 'Y_Maximum', 'Pixels_Areas', 'X_Perimeter', 'Y_Perimeter', 'Sum_of_Luminosity', 'Minimum_of_Luminosity', 'Maximum_of_Luminosity', 'Length_of_Conveyer', 'TypeOfSteel_A300', 'TypeOfSteel_A400', 'Steel_Plate_Thickness', 'Edges_Index', 'Empty_Index', 'Square_Index', 'Outside_X_Index', 'Edges_X_Index', 'Edges_Y_Index', 'Outside_Global_Index', 'LogOfAreas', 'Log_X_Index', 'Log_Y_Index', 'Orientation_Index', 'Luminosity_Index', 'SigmoidOfAreas'])\n",
    "        y = data.iloc[:,27:34]\n",
    "        #Converting 7 columns into one y 'class' column\n",
    "        def fun1(x):\n",
    "            for i in range(len(x)):\n",
    "                if x[i] == 1:\n",
    "                    return i\n",
    "        y1= []        \n",
    "        for j in range(len(y)):        \n",
    "            y1.append((fun1(y.iloc[j]))) \n",
    "        y2 = pd.DataFrame(y1)\n",
    "        y2.columns=['Class']\n",
    "        y=y2\n",
    "        X = self.normalizedata(X)\n",
    "        X_train,X_test,y_train,y_test = self.train_split(X,y)\n",
    "        print(X_train.shape)\n",
    "        print(X_test.shape)\n",
    "        print(y_train.shape)\n",
    "        print(y_test.shape)\n",
    "        flscores = self.train_models(X_train,y_train,X_test,y_test)\n",
    "        print(flscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 540
    },
    "colab_type": "code",
    "id": "8wX73CRexW6A",
    "outputId": "31a57c25-01cf-484c-a0fd-15e5ba811e4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization done\n",
      "(1358, 27)\n",
      "(583, 27)\n",
      "(1358, 1)\n",
      "(583, 1)\n",
      "Knn Grid Search Starting...\n",
      "KNN  report\n",
      "-------------------------------------\n",
      " \n",
      " Confusion Matrix  [[ 27   1   0   0   1  10  12]\n",
      " [  0  53   0   0   0   0   7]\n",
      " [  0   0 108   1   0   0   5]\n",
      " [  0   0   0  17   0   2   1]\n",
      " [  0   0   0   0  13   0   0]\n",
      " [  6   6   0   1   3  94  25]\n",
      " [  9   5   6   3   1  44 122]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.53      0.58        51\n",
      "           1       0.82      0.88      0.85        60\n",
      "           2       0.95      0.95      0.95       114\n",
      "           3       0.77      0.85      0.81        20\n",
      "           4       0.72      1.00      0.84        13\n",
      "           5       0.63      0.70      0.66       135\n",
      "           6       0.71      0.64      0.67       190\n",
      "\n",
      "    accuracy                           0.74       583\n",
      "   macro avg       0.75      0.79      0.77       583\n",
      "weighted avg       0.74      0.74      0.74       583\n",
      "\n",
      " \n",
      "-------------------------------------\n",
      " \n",
      "Best Hyperparameters {'n_neighbors': 8} Best Score: 0.7069219440353461\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-ef89b0ef48dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mentrypoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentryPoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mentrypoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteel_plates_faults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-0e39f4727d2d>\u001b[0m in \u001b[0;36msteel_plates_faults\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0mflscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-0e39f4727d2d>\u001b[0m in \u001b[0;36mtrain_models\u001b[0;34m(self, X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mf1scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mf1scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0mf1scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mknn_grid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0mf1scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mf1scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandomForest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-0e39f4727d2d>\u001b[0m in \u001b[0;36mknn_grid_search\u001b[0;34m(self, X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintaccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mknn_predict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"KNN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best Hyperparameters \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mknn_grid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" Best Score: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mknn_grid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mflScore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mknn_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mflScore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[1;32m   1057\u001b[0m     return fbeta_score(y_true, y_pred, 1, labels=labels,\n\u001b[1;32m   1058\u001b[0m                        \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m                        sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[1;32m   1180\u001b[0m                                                  \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'f-score'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m                                                  sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1183\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[1;32m   1413\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001b[0;32m-> 1415\u001b[0;31m                                     pos_label)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1252\u001b[0m             raise ValueError(\"Target is %s but average='binary'. Please \"\n\u001b[1;32m   1253\u001b[0m                              \u001b[0;34m\"choose another average setting, one of %r.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m                              % (y_type, average_options))\n\u001b[0m\u001b[1;32m   1255\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mpos_label\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         warnings.warn(\"Note that pos_label (set to %r) is ignored when \"\n",
      "\u001b[0;31mValueError\u001b[0m: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted']."
     ]
    }
   ],
   "source": [
    "entrypoint = entryPoint()\n",
    "entrypoint.steel_plates_faults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "General_Program_for_all.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
